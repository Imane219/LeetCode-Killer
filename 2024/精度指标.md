常见目标检测中评价指标可以分为精度指标和速度指标

## 精度指标

### 1.Accuracy

acc = 正确预测的正反例数/总数

### 2.精确率(Precision)与召回率(Recall)

![](C:\D\0\blog\cnblogs\figures\2.jpg)

**正样本:** 当前统计的类别的样本

**负样本:** 所有非当前类别的样本

**True Positives:** 正样本被正确识别为正样本，即模型识别的正样本中真正正样本的数量

**True Negatives:** 负样本被正确识别为负样本，即模型识别的负样本中真正负样本的数量

**False positives:** 假的正样本，即模型识别的正样本中负样本的数量

**False Negatives:** 假的负样本，即模型识别的负样本中正样本的数量

**Precision:** TP/(TP+FP)，在识别出来的图片中，True positives所占的比例

**Recall:** TP/(TP+FN)在所有正样本中，True positives所占的比例

**PR曲线:** 

![](C:\D\0\blog\cnblogs\figures\3.jpg)

如果一个模型性能好，那么它应该有如下性质：

- Recall增长的同时，Precision保持在一个很高的水平(即AP面积大)

性能较差的模型可能损失很多Precision值才能换来Recall的增长。

AP是一条曲线下面积，mAP则是多条的平均，它们均<=1，值越大越好。

**ROC曲线**: receiver operating characteristic curve/sensitive curve

- 纵坐标: **命中率(True Positive Rate, TPR)**，TP/(TP+FN)，所有正样本中预测为正样本的概率(**recall**)

  - 反映**敏感度**

- 横坐标: **假正率(False Positive Rate, FPR)**，FP/(FP+TN)，所有负样本中预测为正样本的概率

  - 反应**1-特异度**(1-Specificity)

  <img src="C:\D\0\blog\cnblogs\figures\4.png" style="zoom:50%;" />

**AUC**: Area under curve, ROC曲线下的面积。

- **数学意义**：AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。AUC越接近1，模型性能越好。

**ROC VS PR**

- ROC曲线
  - 当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例	。
  - 在某种程度上也是**其缺点**。因为负样本N增加了很多，而曲线却没变，这等于产生了大量FP。**像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了**。在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。
- PR曲线
  - PR曲线的两个指标都**聚焦于正例**。**类别不平衡问题中由于主要关心正例**，所以在此情况下PR曲线被广泛认为优于ROC曲线。

**F1-score**

分类问题的一个衡量指标。F1-score认为召回率和精度同等重要，它是精确率和召回率的调和平均数，最大为1，最小为0。
$$
F1= 2TP/(2TP+FP+FN)
$$
此外还有F2-score和F0.5-score。F2-score分数认为召回率的重要程度是精度的2倍，而F0.5-score认为召回率的重要程度是精度的一半。

## 速度指标

### FLOPs和FLOPS

- **FLOPs**: floating point operations，浮点运算次数，理解为计算量，可以用来衡量算法/模型复杂度。
- **FLOPS**: 每秒运算的浮点数，理解为计算速度，衡量一个硬件的标准。

